<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]--><!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8"><![endif]--><!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9"><![endif]--><!--[if gt IE 8]><!--><html class="no-js">
<!--<![endif]--> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"> <title>Machine Learning – Evolving perceptions</title> <meta name="description" content="A space where I pen down my views and experiences"> <meta name="keywords" content="ML, AI, supervised learning, unsupervised learning"> <!-- Twitter Cards --> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="http://127.0.0.1:4000/assets/img/logo.jpg"> <meta name="twitter:title" content="Machine Learning"> <meta name="twitter:description" content="Lets hack on some ML stuff"> <meta name="twitter:site" content="@sailesh_dev"> <meta name="twitter:creator" content="@sailesh_dev"> <!-- Open Graph --> <meta property="og:locale" content="en_US"> <meta property="og:type" content="article"> <meta property="og:title" content="Machine Learning"> <meta property="og:description" content="Lets hack on some ML stuff"> <meta property="og:url" content="http://127.0.0.1:4000/ml/"> <meta property="og:site_name" content="Evolving perceptions"> <meta property="og:image" content="http://127.0.0.1:4000/assets/img/logo.jpg"> <link rel="canonical" href="http://127.0.0.1:4000/ml/"> <link href="http://127.0.0.1:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Evolving perceptions Feed"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- CSS --> <link rel="stylesheet" href="http://127.0.0.1:4000/assets/css/main.css"> <!-- JS --> <script src="http://127.0.0.1:4000/assets/js/modernizr-3.3.1.custom.min.js"></script> <!-- Favicons --> <link rel="apple-touch-icon" href="http://127.0.0.1:4000/assets/img/favicons/apple-icon-precomposed.png"> <link rel="apple-touch-icon" sizes="72x72" href="http://127.0.0.1:4000/assets/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="114x114" href="http://127.0.0.1:4000/assets/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="144x144" href="http://127.0.0.1:4000/assets/img/favicons/apple-icon-144x144.png"> <link rel="shortcut icon" type="image/png" href="http://127.0.0.1:4000/favicon.png"> <link rel="shortcut icon" href="http://127.0.0.1:4000/favicon.ico"> <!-- Background Image --> <style type="text/css">body {background-image:url(http://127.0.0.1:4000/assets/img/placeholder-big.jpg); background-repeat: no-repeat; background-size: cover; }</style> <!-- Post Feature Image --> </head> <body> <nav id="dl-menu" class="dl-menuwrapper" role="navigation"> <button class="dl-trigger">Open Menu</button> <ul class="dl-menu"> <li><a href="http://127.0.0.1:4000/">Home</a></li> <li> <a href="#">About</a> <ul class="dl-submenu"> <li> <img src="http://127.0.0.1:4000/assets/img/logo.jpg" alt="Evolving perceptions photo" class="author-photo"> <h4>Evolving perceptions</h4> <p>A space where I pen down my views and experiences</p> </li> <li><a href="http://127.0.0.1:4000/about/"><span class="btn btn-inverse">Learn More</span></a></li> <li> <a href="http://twitter.com/sailesh_dev" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a> </li> <li> <a href="http://linkedin.com/in/sailesh-dev-2822b77b" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a> </li> <li> <a href="http://github.com/saileshdev" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-github"></i> Github</a> </li> </ul>
<!-- /.dl-submenu --> </li> <li> <a href="#">Posts</a> <ul class="dl-submenu"> <li><a href="http://127.0.0.1:4000/posts/">All Posts</a></li> <li><a href="http://127.0.0.1:4000/tags/">All Tags</a></li> </ul> </li> <li><a href="http://127.0.0.1:4000/projects/">Projects</a></li> </ul>
<!-- /.dl-menu --> </nav><!-- /.dl-menuwrapper --> <!-- Header --> <header class="header" role="banner"> <div class="wrapper animated fadeIn"> <div class="content"> <div class="post-title "> <h1>Machine Learning</h1> <h4>18 Mar 2016</h4> <p class="reading-time"> <i class="fa fa-clock-o"></i> Reading time ~15 minutes </p>
<!-- /.entry-reading-time --> <a class="btn zoombtn" href="http://127.0.0.1:4000/posts/"> <i class="fa fa-chevron-left"></i> </a> </div> <p>In Machine Learning(ML), computers apply statistical learning techniques to automatically identify patterns in data. These techniques can be used to make highly accurate predictions.</p> <h3 id="what-is-ml-all-about">What is ML all about?</h3> <blockquote> <p>Algorithms that can learn from observational data, and can make predictions on it.</p> </blockquote> <hr> <h2 id="learning">Learning:</h2> <ol> <li>Unsupervised Learning</li> <li>Supervised Learning</li> </ol> <h3 id="unsupervised-learning">Unsupervised learning:</h3> <p>The model is not given any answers to learn from, it must make sense of data just given the observations themselves. We are looking for latent variables(some property we didnt know existed originally)</p> <h3 id="supervised-learning">Supervised learning:</h3> <p>The data the algorithm learns from comes with the correct “answers”. We give a training data from which the model can learn. The model created is then used to predict the answer for new, unknown values Example: We can train a model for predicting car prices based on car attributes using historical sales data. The model can then predict the optimal price for new cars that havent been sold before.</p> <h3 id="reality">Reality</h3> <p>To illustrate we can create the data sets ourselves(random samples), like we created 2 variables with data that depends linearly on each other. But in reality this is not the case. So what do we do?</p> <p>We collect a set of “answers”(ie correct data), ie lets say we are predicting car prices, so we collect a set of “answers”, ie historical sales data. Now we perform our regression on this data(we call this data as the training data)and try to fit a curve.</p> <p>Regression can be thought as a supervised machine learning.</p> <hr> <h2 id="evaluating-supervised-learning">Evaluating supervised learning:</h2> <p>If we have a set of training data that includes the values we are trying to predict, we split the data into two data sets:</p> <ul> <li>A training set</li> <li>A test set</li> </ul> <p>We then train the model using training set. We then measure(using r-squared or some other metric) the models accuracy by asking it to predict values for the test set and comparethat to known true values.</p> <p>So the beauty of supervised learning is this concept of train/test where we split the training data(“correct” answers) into 2 sets, namely train set and test set. This helps us to prevent overfitting.</p> <hr> <h2 id="testtrain-in-practice">Test/Train in practice:</h2> <ul> <li>Need to ensure both sets are large enough to contain representives of all the variances and outliners</li> <li>The data sets must be selected <strong><em>randomly</em></strong>(ie partition of the data sets into 2 train and test must be done randomly)</li> <li>Train/test is a great way to guard against overfitting</li> </ul> <p>But still can give us overfitting, say if the sample sizes are too small. But ther is a way around that too to protect against overfitting, its called K-fold cross validation.</p> <hr> <h2 id="k-fold-cross-validationway-to-protect-against-overfitting">K-fold Cross Validation(Way to protect against overfitting):</h2> <ol> <li>We split the data into k segments and not just 2 datasets</li> <li>Now we take one as the test set and the remaining as training sets</li> <li>Now we train the multiple data sets and individually measure thier performance against the test data set</li> <li>Now we take the average performance And find the r-squared average score (Train/test can still give us overfitting and K-fold method protects that limitation. Here we have multiple training sets so even if the result of one training set is overfitting then the others will average it out)</li> </ol> <hr> <h2 id="traintest-in-python">Train/Test in python</h2> <p>Regression is a form of supervised machine learning. So lets take a polynomial regression example and use train/test to find out the right degree polynomial for the data set.</p> <p>We will start by creating some data set that we want to build a model for (in this case a polynomial regression):</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pylab</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">pageSpeeds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">purchaseAmount</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">50.0</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="o">/</span> <span class="n">pageSpeeds</span>

<span class="n">scatter</span><span class="p">(</span><span class="n">pageSpeeds</span><span class="p">,</span> <span class="n">purchaseAmount</span><span class="p">)</span>
</code></pre></div> <p>Now we will split the data in two - 80% of it will be used for “training” our model, and the other 20% for testing it. This way we can avoid overfitting.</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">trainX</span> <span class="o">=</span> <span class="n">pageSpeeds</span><span class="p">[:</span><span class="mi">80</span><span class="p">]</span>
<span class="n">testX</span> <span class="o">=</span> <span class="n">pageSpeeds</span><span class="p">[</span><span class="mi">80</span><span class="p">:]</span>

<span class="n">trainY</span> <span class="o">=</span> <span class="n">purchaseAmount</span><span class="p">[:</span><span class="mi">80</span><span class="p">]</span>
<span class="n">testY</span> <span class="o">=</span> <span class="n">purchaseAmount</span><span class="p">[</span><span class="mi">80</span><span class="p">:]</span>
<span class="s">""" We actually have to randomize the selectioon of train set and test set. So we need to shuffle the data. It can do it with random.shuffle method.
But in this case it is not required as the original data is itself randomized and hence slicing will also be random """</span>
</code></pre></div> <p>Here is our training dataset: <code class="highlighter-rouge">scatter(trainX, trainY)</code></p> <p>And our test dataset: <code class="highlighter-rouge">scatter(testX, testY)</code></p> <p>Now we will try to fit an 8th-degree polynomial to this data (which is almost certainly overfitting, given what we know about how it was generated!)</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trainX</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trainY</span><span class="p">)</span>

<span class="n">p4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</code></pre></div> <p>Lets plot our polynomial(best fit curve) against the training data:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">xp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">7</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">p4</span><span class="p">(</span><span class="n">xp</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div> <p>Lets plot our polynomial(best fit curve) against the training data:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">testx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
<span class="n">testy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">testY</span><span class="p">)</span>

<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">7</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">testx</span><span class="p">,</span> <span class="n">testx</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">p4</span><span class="p">(</span><span class="n">xp</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div> <p>Lets now compute the r-squared score to check the quality of our best fit</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>

<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">testy</span><span class="p">,</span> <span class="n">p4</span><span class="p">(</span><span class="n">testx</span><span class="p">))</span>
<span class="c"># see how we are using the test data set to test the model that we have trained from our train set</span>

<span class="k">print</span> <span class="n">r2</span>
<span class="c"># returns -2.62352440155e+24</span>
</code></pre></div> <p>The r-squared score on the test data is kind of horrible! This tells us that our model isnt all that great.</p> <p><strong><em>Note:</em></strong></p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>

<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trainY</span><span class="p">),</span> <span class="n">p4</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trainX</span><span class="p">)))</span>

<span class="k">print</span> <span class="n">r2</span>
<span class="c">#returns 0.642706951469</span>
</code></pre></div> <p>It fits the training set better thats why gives us a r-squared score of 0.64 on the training set. So even though this model fits the training set, it doesnt fit the test set(r-squared score is -2.62). Thats the beauty of train/test model which helps us to figure out the best-fit for predictions.</p> <p>If we are working with a Pandas DataFrame (using tabular, labeled data,) scikit-learn has built-in train_test_split functions to make random splitting of training data into test set and train set easy to do.</p> <hr> <h2 id="bayesian-methods">Bayesian methods:</h2> <p>Ex:Spam classifier in mail</p> <h3 id="naive-bayse">Naive bayse:</h3> <p><strong><em>How it works?</em></strong> Ex: Lets express the probability of an email being spam if it contains the word “free”</p> <p><code class="highlighter-rouge">p(spam given free) = p(spam)p(free given sample)/p(free)</code> The numerator is probability of a message being spam and containing the word “free” The denominator is the overall probability of an email containing the word free</p> <h3 id="what-about-the-others-words">What about the others words?</h3> <ul> <li>We construct p(spam given word) for every meaning word we encounter during training</li> <li>We then multiply these together when anlayzing a new email to get the probability of it being spam It is called naive bayse because we assume there is no relationship between each words we just assume that each word is independent</li> </ul> <hr> <h2 id="spam-classifier-using-python">Spam classifier using python:</h2> <ul> <li>Use scikit-learn</li> <li>The CountVectorizer lets us operate on lots of words at once, and MultinomialNB does all the heavy lifting on Naive Bayse</li> </ul> <p>We will train it on known set of spams and hams(non-spams),so this is supervised learning</p> <p>Lets load our training data into a pandas DataFrame that we can play with:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>

<span class="k">def</span> <span class="nf">readFiles</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">root</span><span class="p">,</span> <span class="n">dirnames</span><span class="p">,</span> <span class="n">filenames</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">walk</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">:</span>
                    <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>

                    <span class="n">inBody</span> <span class="o">=</span> <span class="bp">False</span>
                    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="n">f</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'latin1'</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">inBody</span><span class="p">:</span>
                            <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                        <span class="k">elif</span> <span class="n">line</span> <span class="o">==</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">:</span>
                            <span class="n">inBody</span> <span class="o">=</span> <span class="bp">True</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
                    <span class="n">message</span> <span class="o">=</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
                    <span class="k">yield</span> <span class="n">path</span><span class="p">,</span> <span class="n">message</span>

<span class="k">def</span> <span class="nf">dataFrameFromDirectory</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">classification</span><span class="p">):</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">index</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">filename</span><span class="p">,</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">readFiles</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s">'message'</span><span class="p">:</span> <span class="n">message</span><span class="p">,</span> <span class="s">'class'</span><span class="p">:</span> <span class="n">classification</span><span class="p">})</span>
        <span class="n">index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">({</span><span class="s">'message'</span><span class="p">:</span> <span class="p">[],</span> <span class="s">'class'</span><span class="p">:</span> <span class="p">[]})</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dataFrameFromDirectory</span><span class="p">(</span><span class="s">'full_path/emails/spam'</span><span class="p">,</span> <span class="s">'spam'</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dataFrameFromDirectory</span><span class="p">(</span><span class="s">'/full_path/emails/ham'</span><span class="p">,</span> <span class="s">'ham'</span><span class="p">))</span>
</code></pre></div> <p>Lets have a look at that DataFrame: <code class="highlighter-rouge">data.head()</code></p> <p>Now we will use a CountVectorizer to split up each message into its list of words, and throw that into a MultinomialNB classifier. Call fit() and we have got a trained spam filter ready to go!</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'message'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="c">#data['message'].values gives back an ndarray which has values of the message column</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'class'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
</code></pre></div> <p>Now lets try predicting new mails:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">examples</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Earn free house now!!!'</span><span class="p">,</span> <span class="s">"Hi Sailesh, how about a cup of coffee tomorrow?"</span><span class="p">]</span>
<span class="n">example_counts</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">example_counts</span><span class="p">)</span>

<span class="s">""" prediction contains:
array(['spam', 'ham'], 
      dtype='|S4')
"""</span>
</code></pre></div> <p>So the first mail in the examples list is a spam and the second mail is a ham.</p> <hr> <h2 id="k-means-clusteringunsupervised-learning">K-means clustering(Unsupervised learning):</h2> <p>Where we want to group data into clusters may be movie genres or demographics of people . It attempts to split data into k groups that are closest to K centroids Unsupervised learning - uses only positions of each data point Examples: Where do millionaires lives? What genres of movies naturally fall out of data?</p> <h3 id="procedure">Procedure:</h3> <ol> <li>First it chooses K random centroids</li> <li>Then it finds the distance between each point and its centroid and assigns cluster for it</li> <li>Now in each region it calculates the effective centroid Repeat step 2 Keep doing it until you narrow down or the nth iteration centroid are almost same as the n-1th centroid(until we converge).</li> </ol> <h3 id="limitations">Limitations:</h3> <ul> <li> <p>Choosing K is hard. How can we choose K? One solution is to increase K values until we stop getting large reductions in squared error(distances from each point to thier centroids)</p> </li> <li>Choice of initial centroids <ol> <li>The random choice of initial centroids can yield different results</li> <li>Run it a few times to make sure our initail solutions are not wacky</li> </ol> </li> <li>Labelling the clusters <ol> <li>K-means does not attempt to assign any meaning to the clusters we find</li> <li>Its upto us to try to dig into the data and try to determine that</li> </ol> </li> </ul> <hr> <h2 id="k-means-clustering-using-python">K-means clustering using python</h2> <p>Lets make some fake data that includes people clustered by income and age, randomly:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">random</span><span class="p">,</span> <span class="n">array</span>

<span class="c">#Create fake income/age clusters for N people in k clusters</span>

<span class="k">def</span> <span class="nf">createClusteredData</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">pointsPerCluster</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">/</span><span class="n">k</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">incomeCentroid</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">20000.0</span><span class="p">,</span> <span class="mf">200000.0</span><span class="p">)</span>
        <span class="n">ageCentroid</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">20.0</span><span class="p">,</span> <span class="mf">70.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">pointsPerCluster</span><span class="p">)):</span>
            <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">incomeCentroid</span><span class="p">,</span> <span class="mf">10000.0</span><span class="p">),</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">ageCentroid</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)])</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span>
</code></pre></div> <p>We will now use k-means to rediscover these clusters in unsupervised learning:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">random</span><span class="p">,</span> <span class="nb">float</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">createClusteredData</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c"># Note here we are scaling the data to normalize it! Important for good results.</span>
<span class="c"># ages range from 20 to 70 while incomes range from 2000 to 200000, so scale will normalize it</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

<span class="c"># We can look at the clusters each data point was assigned to</span>
<span class="k">print</span> <span class="n">model</span><span class="o">.</span><span class="n">labels_</span> 

<span class="c"># And we will visualize it:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">labels_</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div> <hr> <h2 id="entropy">Entropy:</h2> <p>A measure of disorder of a data set, how same or different is the data set Entropy is 0 if all the classes in the data are the same. The entropy is high if they all are different.</p> <hr> <h2 id="decision-tree">Decision tree:</h2> <ul> <li>We can train on a training data to generate a flow chart to make a decision from</li> <li>We can actually construct a flow chart to help us decide for a classification with machine learning(called decision tree)</li> <li>Good technique when we have a lot of attributes to classify something</li> <li>Another form of supervised learning</li> <li>Most interesting application of machine learning</li> <li>Gives a flow chart of how to make a decision</li> </ul> <p>Give it some sample data and the resulting classification and out comes a tree!</p> <h3 id="decision-tree-example">Decision tree example:</h3> <p>Lets build a system to filter out new resumes based on historical data</p> <ol> <li>So we are going to form a decision tree which takes a resume and finds out if the person is suitable for the job</li> <li>We train the decision tree based on the historical data, which is a database of some important atrributes of job candidates and we know which ones were hired and which ones werent</li> <li>We then predict whether a new candidate will get hired based on it</li> </ol> <h3 id="how-does-a-decision-tree-work">How does a decision tree work?</h3> <ol> <li>At each step, find the attribute we can use to partition the data set to minimize the entropy of the data at the next step</li> <li>Make the dat more and more uniform as we traverse down the flowchart</li> </ol> <p>Fancy name for the algorithm - ID3. It is a greedy algorithm</p> <h3 id="limitation">Limitation?</h3> <p>Decision tress are very susceptible to overfitting</p> <h3 id="how-to-overcomes-this">How to overcomes this?</h3> <p>We overcome this by a method called <strong><em>Random forests</em></strong></p> <ol> <li>We select a random samples of data from the training set</li> <li>And for each of these a decision tree is formed. Each tree uses a different sets of attributes, making the algorithm much better.</li> <li>Now we let them “vote” on the final decision tree. The process of randomly selecting multiple data sets(ie randomly resampling the input data) is called <strong><em>bootstrap aggregating</em></strong> or <strong><em>bagging</em></strong>
</li> </ol> <hr> <h2 id="decision-trees-using-python">Decision trees using python:</h2> <p>First we will load some data set. Note how we use pandas to convert a csv file into a DataFrame:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>

<span class="c"># we specify the full path of the historical data in the input_file variable</span>
<span class="n">input_file</span> <span class="o">=</span> <span class="s">"full_path/data_set.csv"</span>

<span class="c"># we create a data frame object from the input file</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">input_file</span><span class="p">,</span> <span class="n">header</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c">#to see the first few rows of the data set call df.head()</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div> <p>Firstly we need to convert the values in the data frame to numbers as scikit lib works only with numers and math and not alphabets or words. So, we will map categorical data to numerical values. In the real world, we need to think about how to deal with unexpected or missing data!</p> <p>By using map(), we know we will get NaN for unexpected values.</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="n">n</span><span class="p">])</span>
<span class="c">#features is now ['predictor 1','predictor 2','predictor 3' ....,'predictor n']</span>

<span class="c">#we now actually consstruct the decision tree</span>
<span class="c">#lets assign y to be what we are trying to predict,ie target column</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"target_column"</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">features</span><span class="p">]</span> <span class="c">#collection of all of the data of all of the feature columns</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span> <span class="c">#we create the classifier first</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c">#now we use the classifier to fit the data</span>
</code></pre></div> <p>Now lets print the decision tree,</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>  
<span class="kn">from</span> <span class="nn">sklearn.externals.six</span> <span class="kn">import</span> <span class="n">StringIO</span>  
<span class="kn">import</span> <span class="nn">pydot</span> 

<span class="n">dot_data</span> <span class="o">=</span> <span class="n">StringIO</span><span class="p">()</span>  
<span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="n">dot_data</span><span class="p">,</span>  
                        <span class="n">feature_names</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>  
<span class="n">graph</span> <span class="o">=</span> <span class="n">pydot</span><span class="o">.</span><span class="n">graph_from_dot_data</span><span class="p">(</span><span class="n">dot_data</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span>  
<span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">create_png</span><span class="p">())</span> 
</code></pre></div> <hr> <h2 id="random-forests-using-python">Random forests using python:</h2> <p>We will use a random forest of 10 decision trees to predict employment of specific candidate profiles:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="s">"""when we are dealing with random forests, we dont have to walk throught the tree by hand, 
we use the predict function and pass in the row dataframe"""</span>

<span class="c">#Predict employment of an employed 10-year veteran</span>
<span class="k">print</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="c">#Predict employment of an unemployed 10-year veteran</span>
<span class="k">print</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="c">#we get the result as 0 or 1 </span>
</code></pre></div> <p>Random forests is essentially an ensemble learning technique.</p> <hr> <h2 id="ensemble-learning">Ensemble learning:</h2> <p>It just means we use multiple models to try and solve the same problem and let them vote on the results Random forests is an example of ensemble learning</p> <p>Random forests uses bagging(bootstrap aggregating) to implement ensemble learning. Meaning many models are built, by training on randomly-drawn subsets of the data.</p> <p>Boosting is an alternate technique. Here each subsequent model in the ensemble boosts attributes that address data mis-classified by the previous model</p> <p>A bucket of models, ie several different models train on the training data and picks the one that works best with the test data</p> <p>Stacking runs multiple models at one on the data and combine the result together(This is how Netflix prize was won!)</p> <hr> <h2 id="advanced-ensemble-learning">Advanced ensemble learning:</h2> <h3 id="bayes-optimal-classifier">Bayes Optimal Classifier</h3> <ul> <li>Theoritically the best, but almost always impractical</li> </ul> <h3 id="bayesian-parametric-averaging">Bayesian Parametric Averaging</h3> <ul> <li>Attempts to make BOC practical, but its still misunderstood, susceptible to overfitting and often outperformed by the simpler bagging approach</li> </ul> <h3 id="bayesian-model-combination">Bayesian Model Combination</h3> <ul> <li>Tries to address all those problems</li> <li>But in the end its all about the same as using cross-validation to find the best combination of models.</li> </ul> <hr> <h2 id="support-vector-machinessvm">Support Vector Machines(SVM)</h2> <p>Works well for classifying higher dimensional data(lots of features)</p> <p>K-means clustering worked on 2 dimensional data, ie age and income, while this can work on higher dimensional data. It can be used to cluster/classify data sets with a lot of features.</p> <ul> <li>Finds higher dimensional support vectors accross which to divide the data(mathematically these support vectors define hyperplanes)</li> <li>Underlying, it uses kernel trick to represent the data in higher dimensional spaces to find hyperplanes</li> </ul> <p>SVM is a supervised learning technique. So we train it on a set of training data to predict new data. So its differnt drom K-means clustering(which is an unsupervised learning). SVM need training data, ie set of “correct” classifications to learn from.</p> <h3 id="support-vector-classificationsvc">Support Vector classification(SVC):</h3> <ol> <li>SVC is nothing but classifying data using SVM</li> <li>We can use different kernels with SVC. Some work better than others for a given data set.</li> </ol> <hr> <h2 id="svc-using-python">SVC using python</h2> <p>Lets create the same fake income / age clustered data that we used for our K-Means clustering example:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c">#Create fake income/age clusters for N people in k clusters</span>
<span class="k">def</span> <span class="nf">createClusteredData</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">pointsPerCluster</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">/</span><span class="n">k</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">incomeCentroid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">20000.0</span><span class="p">,</span> <span class="mf">200000.0</span><span class="p">)</span>
        <span class="n">ageCentroid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">20.0</span><span class="p">,</span> <span class="mf">70.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">pointsPerCluster</span><span class="p">)):</span>
            <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">incomeCentroid</span><span class="p">,</span> <span class="mf">10000.0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">ageCentroid</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)])</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</code></pre></div> <p>Lets plot it:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pylab</span> <span class="kn">import</span> <span class="o">*</span>

<span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">createClusteredData</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">float</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div> <p>Now we will use linear SVC to partition our graph into clusters:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span><span class="p">,</span> <span class="n">datasets</span>

<span class="n">C</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c"># we are using the linear kernel here in this case</span>
</code></pre></div> <p>By setting up a dense mesh of points in the grid and classifying all of them, we can render the regions of each cluster as distinct colors:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plotPredictions</span><span class="p">(</span><span class="n">clf</span><span class="p">):</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">250000</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">float</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div> <p>Or just use predict for a given point:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">svc</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">200000</span><span class="p">,</span> <span class="mi">40</span><span class="p">]])</span>
<span class="c">#returns array([4])</span>
<span class="c"># means a person of age 40 with 200000 income falls under category 4</span>
</code></pre></div> <hr> <h2 id="recommender-systems">Recommender systems:</h2> <p>Systems which can recommend stuff to people based on what everybody else did</p> <h3 id="user-based-collaborative-filtering">User-based collaborative filtering:</h3> <ol> <li>Build a matrix of things each user bought/viewed/rated</li> <li>Compute similarity scores between users</li> <li>Find usrers similar to you Recommend stuff they have bought/viewed/rated that you haven’t yet</li> </ol> <h3 id="problems-with-user-based-cf">Problems with User-based CF:</h3> <ol> <li>People are fickle. Their tastes change.</li> <li>There are usually many more people that things(By focusing on finding similar people versus similar things, we are making the computational problem harder)</li> <li>People do crazy things(shilling attacks).</li> </ol> <div class="entry-meta"> <br> <hr> <span class="entry-tags"><a href="http://127.0.0.1:4000/tags/#ML" title="Pages tagged ML" class="tag"><span class="term">ML</span></a><a href="http://127.0.0.1:4000/tags/#AI" title="Pages tagged AI" class="tag"><span class="term">AI</span></a><a href="http://127.0.0.1:4000/tags/#supervised%20learning" title="Pages tagged supervised learning" class="tag"><span class="term">supervised learning</span></a><a href="http://127.0.0.1:4000/tags/#unsupervised%20learning" title="Pages tagged unsupervised learning" class="tag"><span class="term">unsupervised learning</span></a></span> <span class="social-share"> <a href="https://www.facebook.com/sharer/sharer.php?u=http://127.0.0.1:4000/ml/" title="Share on Facebook" class="tag"> <span class="term"><i class="fa fa-facebook-square"></i> Like</span> </a> <a href="https://twitter.com/intent/tweet?text=http://127.0.0.1:4000/ml/" title="Share on Twitter" class="tag"> <span class="term"><i class="fa fa-twitter-square"></i> Tweet</span> </a> <a href="https://plus.google.com/share?url=http://127.0.0.1:4000/ml/" title="Share on Google+" class="tag"> <span class="term"><i class="fa fa-google-plus-square"></i> +1</span> </a> </span> <div style="clear:both"></div> </div> </div> </div> <section id="disqus_thread" class="animated fadeInUp"></section><!-- /#disqus_thread --> </header> <!-- JS --> <script src="http://127.0.0.1:4000/assets/js/jquery-1.12.0.min.js"></script> <script src="http://127.0.0.1:4000/assets/js/jquery.dlmenu.min.js"></script> <script src="http://127.0.0.1:4000/assets/js/jquery.goup.min.js"></script> <script src="http://127.0.0.1:4000/assets/js/jquery.magnific-popup.min.js"></script> <script src="http://127.0.0.1:4000/assets/js/jquery.fitvid.min.js"></script> <script src="http://127.0.0.1:4000/assets/js/scripts.js"></script> <script type="text/javascript"> var disqus_shortname = 'evolvingperceptionsblog'; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); (function () { var s = document.createElement('script'); s.async = true; s.type = 'text/javascript'; s.src = '//' + disqus_shortname + '.disqus.com/count.js'; (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s); }()); </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript> <!-- MathJax --> <script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </body> </html>
